_Fabrizio Cominetti_

University of Milano-Bicocca

Master's Thesis | Data Science | A.Y. 2022/23

<h1 align="center"><b>Exposing Bias in Vision-Language Models</b></h1>

<p align="center"><i>Abstract</i></p>

Multimodal models, capable of incorporating two or more modalities in their processes, have brought about significant and rapid technological innovations in recent years. Among these, vision and language models excel at processing, comprehending, and manipulating data and information from two modalities: text and image, representing textual and visual components, respectively. However, the possible presence of bias represents a significant concern that can overshadow their impressive capabilities.
This paper explores the intricate relationship between vision and language models and bias, examining its manifestations, causes, and implications. Starting with the construction of a novel and morphed dataset derived from UTKFace, this paper utilizes three pre-trained models - ALBEF, BLIP-2, and CLIP - to investigate gender and racial biases. The experiments span from zero-shot retrieval to zero-shot classification tasks, aiming to identify and measure bias across specific demographic groups. The research questions focus on the extent of race and gender bias in these tasks, and fairness metrics are employed for quantitative assessment.
The findings contribute to advancing fair artificial intelligence by emphasizing the importance of addressing biases, particularly within these increasingly adopted models.